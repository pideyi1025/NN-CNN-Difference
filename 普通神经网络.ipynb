{"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets(\"data/\", one_hot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<img src=\"./img/4.png\" alt=\"FAO\" width=\"790\">","metadata":{}},{"cell_type":"markdown","source":"单层神经网络","metadata":{}},{"cell_type":"markdown","source":"<img src=\"./img/5.png\" alt=\"FAO\" width=\"790\">","metadata":{}},{"cell_type":"markdown","source":"参数设置","metadata":{}},{"cell_type":"code","source":"numClasses = 10 \ninputSize = 784 \nnumHiddenUnits = 50 \ntrainingIterations = 10000 \nbatchSize = 100 ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = tf.placeholder(tf.float32, shape = [None, inputSize])\ny = tf.placeholder(tf.float32, shape = [None, numClasses])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"参数初始化","metadata":{}},{"cell_type":"code","source":"W1 = tf.Variable(tf.truncated_normal([inputSize, numHiddenUnits], stddev=0.1))\nB1 = tf.Variable(tf.constant(0.1), [numHiddenUnits])\nW2 = tf.Variable(tf.truncated_normal([numHiddenUnits, numClasses], stddev=0.1))\nB2 = tf.Variable(tf.constant(0.1), [numClasses])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"网络结构","metadata":{}},{"cell_type":"code","source":"hiddenLayerOutput = tf.matmul(X, W1) + B1\nhiddenLayerOutput = tf.nn.relu(hiddenLayerOutput)\nfinalOutput = tf.matmul(hiddenLayerOutput, W2) + B2\nfinalOutput = tf.nn.relu(finalOutput)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"网络迭代","metadata":{}},{"cell_type":"code","source":"loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = finalOutput))\nopt = tf.train.GradientDescentOptimizer(learning_rate = .1).minimize(loss)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct_prediction = tf.equal(tf.argmax(finalOutput,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor i in range(trainingIterations):\n    batch = mnist.train.next_batch(batchSize)\n    batchInput = batch[0]\n    batchLabels = batch[1]\n    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n    if i%1000 == 0:\n        trainAccuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n        print (\"step %d, training accuracy %g\"%(i, trainAccuracy))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"两层神经网络","metadata":{}},{"cell_type":"code","source":"\nnumHiddenUnitsLayer2 = 100\ntrainingIterations = 10000\n\nX = tf.placeholder(tf.float32, shape = [None, inputSize])\ny = tf.placeholder(tf.float32, shape = [None, numClasses])\n\nW1 = tf.Variable(tf.random_normal([inputSize, numHiddenUnits], stddev=0.1))\nB1 = tf.Variable(tf.constant(0.1), [numHiddenUnits])\nW2 = tf.Variable(tf.random_normal([numHiddenUnits, numHiddenUnitsLayer2], stddev=0.1))\nB2 = tf.Variable(tf.constant(0.1), [numHiddenUnitsLayer2])\nW3 = tf.Variable(tf.random_normal([numHiddenUnitsLayer2, numClasses], stddev=0.1))\nB3 = tf.Variable(tf.constant(0.1), [numClasses])\n\nhiddenLayerOutput = tf.matmul(X, W1) + B1\nhiddenLayerOutput = tf.nn.relu(hiddenLayerOutput)\nhiddenLayer2Output = tf.matmul(hiddenLayerOutput, W2) + B2\nhiddenLayer2Output = tf.nn.relu(hiddenLayer2Output)\nfinalOutput = tf.matmul(hiddenLayer2Output, W3) + B3\n\nloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = finalOutput))\nopt = tf.train.GradientDescentOptimizer(learning_rate = .1).minimize(loss)\n\ncorrect_prediction = tf.equal(tf.argmax(finalOutput,1), tf.argmax(y,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\nsess = tf.Session()\ninit = tf.global_variables_initializer()\nsess.run(init)\n\nfor i in range(trainingIterations):\n    batch = mnist.train.next_batch(batchSize)\n    batchInput = batch[0]\n    batchLabels = batch[1]\n    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n    if i%1000 == 0:\n        train_accuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n        print (\"step %d, training accuracy %g\"%(i, train_accuracy))\n\ntestInputs = mnist.test.images\ntestLabels = mnist.test.labels\nacc = accuracy.eval(session=sess, feed_dict = {X: testInputs, y: testLabels})\nprint(\"testing accuracy: {}\".format(acc))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":2}